version: "3.7"

x-minio-common: &minio-common
  image: quay.io/minio/minio:RELEASE.2023-02-22T18-23-45Z
  command: server --console-address ":9001" http://minio{1...4}/data{1...2}
  environment:
    - MINIO_ROOT_USER=admin
    - MINIO_ROOT_PASSWORD=sample_key
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 30s
    timeout: 20s
    retries: 3
  networks:
    - server

services:
  jupyter:
    hostname: jupyter
    image: helderprado/jupyter-stack-server
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - PYSPARK_SUBMIT_ARGS= --master spark://spark-master:7077 --packages com.amazonaws:aws-java-sdk-bundle:1.11.819,org.apache.hadoop:hadoop-aws:3.2.3 pyspark-shell
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=sample_key
      - MLFLOW_S3_ENDPOINT_URL=http://minio1:9000
    ports:
      - "8888:8888"
    entrypoint:
      - bash
    command:
      - -c
      - |
        jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser --NotebookApp.notebook_dir='/home/jovyan/work' --NotebookApp.token=''
    volumes:
      - ./notebooks:/home/jovyan/work
    networks:
      - server

  db:
    image: mysql/mysql-server:5.7.28
    environment:
      - MYSQL_DATABASE=mlflow
      - MYSQL_USER=mlflow_user
      - MYSQL_PASSWORD=mlflow_password
      - MYSQL_ROOT_PASSWORD=toor
    volumes:
      - db_volume:/var/lib/mysql
    networks:
      - server

  spark-master:
    image: helderprado/spark-stack-server
    hostname: spark-master
    ports:
      - 4040:4040
      - 6066:6066
      - 7077:7077
      - 8080:8080
    volumes:
      - ./notebooks:/home/jovyan/work:rw
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
    networks:
      - server

  spark-worker-a:
    hostname: spark-worker-a
    image: helderprado/spark-stack-server
    ports:
      - 8081:8081
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    volumes:
      - ./notebooks:/home/jovyan/work:rw
    networks:
      - server

  spark-worker-b:
    hostname: spark-worker-b
    image: helderprado/spark-stack-server
    ports:
      - 8082:8082
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    volumes:
      - ./notebooks:/home/jovyan/work:rw
    networks:
      - server

  postgres:
    image: postgres:9.6
    ports:
      - "15432:5432"
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow_db:/var/lib/postgresql/data
    networks:
      - server

  airflow-webserver:
    image: helderprado/airflow-stack-server
    networks:
      - server
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=sample_key
      - AWS_DEFAULT_REGION=us-east-1
      - PYSPARK_SUBMIT_ARGS= --master spark://spark-master:7077 --packages com.amazonaws:aws-java-sdk-bundle:1.11.819,org.apache.hadoop:hadoop-aws:3.2.3 pyspark-shell
      - MLFLOW_S3_ENDPOINT_URL=http://minio1:9000
    volumes:
      - ./notebooks:/usr/local/airflow/dags
    ports:
      - "8282:8282"
    command: webserver

  minio1:
    <<: *minio-common
    hostname: minio1
    volumes:
      - data1-1:/data1
      - data1-2:/data2

  minio2:
    <<: *minio-common
    hostname: minio2
    volumes:
      - data2-1:/data1
      - data2-2:/data2

  minio3:
    <<: *minio-common
    hostname: minio3
    volumes:
      - data3-1:/data1
      - data3-2:/data2

  minio4:
    <<: *minio-common
    hostname: minio4
    volumes:
      - data4-1:/data1
      - data4-2:/data2

  mlflow:
    image: helderprado/mlflow-stack-server
    ports:
      - "5000:5000"
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=sample_key
      - AWS_DEFAULT_REGION=us-east-1
      - MLFLOW_S3_ENDPOINT_URL=http://minio1:9000
    entrypoint: bash ./wait-for-it.sh db:3306 -t 90 -- mlflow server --backend-store-uri mysql+pymysql://mlflow_user:mlflow_password@db:3306/mlflow --default-artifact-root s3://mlflow/ -h 0.0.0.0
    networks:
      - server

  nginx:
    image: nginx:1.19.2-alpine
    hostname: nginx
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "9000:9000"
      - "9001:9001"
    depends_on:
      - minio1
      - minio2
      - minio3
      - minio4
    networks:
      - server

networks:
  server:

volumes:
  airflow_db:
  db_volume:
  data1-1:
  data1-2:
  data2-1:
  data2-2:
  data3-1:
  data3-2:
  data4-1:
  data4-2:
