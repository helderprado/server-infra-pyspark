{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b76078-c281-44f2-9aa9-b4d666eb9ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark inicializado com sucesso com as credenciais do S3\n",
      "CPU times: user 1.86 s, sys: 518 ms, total: 2.38 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import findspark\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"queries\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"sample_key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.proxy.host\", \"s3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://s3:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.port\", \"9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark inicializado com sucesso com as credenciais do S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec31e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
