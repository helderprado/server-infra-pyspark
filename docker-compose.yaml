version: "3.6"

services:
  jupyter:
    build: ./jupyter
    container_name: JupyterLab
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - PYSPARK_SUBMIT_ARGS=--packages com.amazonaws:aws-java-sdk-bundle:1.11.819,org.apache.hadoop:hadoop-aws:3.2.3 pyspark-shell
      - PASSWORD=rstudio1
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:9000
    ports:
      - "9999:9999"
    entrypoint:
      - bash
    command:
      - -c
      - |
        jupyter lab --ip=0.0.0.0 --port=9999 --allow-root --no-browser --NotebookApp.notebook_dir='/home/jovyan/work' --NotebookApp.token=''
    volumes:
      - ./notebooks:/home/jovyan/work
    networks:
      - server

  s3:
    image: minio/minio:RELEASE.2021-11-24T23-19-33Z
    container_name: MinIO
    restart: unless-stopped
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${AWS_ACCESS_KEY_ID}
      - MINIO_ROOT_PASSWORD=${AWS_SECRET_ACCESS_KEY}
    command: server /spark/data --console-address ":9001"
    volumes:
      - minio_volume:/spark/data
    networks:
      - server

  db:
    image: mysql/mysql-server:5.7.28
    restart: unless-stopped
    container_name: MlFlowDatabase
    expose:
      - "3306"
    environment:
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    volumes:
      - db_volume:/var/lib/mysql
    networks:
      - server

  mlflow:
    container_name: MlFlow
    image: tracker_ml
    restart: unless-stopped
    build:
      context: ./mlflow
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_REGION}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:9000
    entrypoint: bash ./wait-for-it.sh db:3306 -t 90 -- mlflow server --backend-store-uri mysql+pymysql://${MYSQL_USER}:${MYSQL_PASSWORD}@db:3306/${MYSQL_DATABASE} --default-artifact-root s3://${AWS_BUCKET_NAME}/ -h 0.0.0.0
    networks:
      - server

  create_s3_buckets:
    container_name: CreateS3Bucket
    image: minio/mc
    depends_on:
      - "s3"
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set minio http://s3:9000 '${AWS_ACCESS_KEY_ID}' '${AWS_SECRET_ACCESS_KEY}') do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb minio/mlflow;
      exit 0;
      "
    networks:
      - server

  postgres:
    container_name: AirFlowDatabase
    image: postgres:9.6
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - /tmp/postgres-data:/var/lib/postgresql/spark/data
    networks:
      - server

  airflow-webserver:
    container_name: AirFlow
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    networks:
      - server
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_REGION}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:9000
    volumes:
      - ./shared:/usr/local/airflow/dags #DAG folder
      - ./shared:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
      - ./shared:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
    ports:
      - "8282:8282"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3

  spark-master:
    container_name: SparkMaster
    build:
      context: ./spark/build
      dockerfile: Dockerfile
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
    networks:
      - server

  spark-worker-a:
    container_name: SparkWorkerA
    build:
      context: ./spark/build
      dockerfile: Dockerfile
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - server

  spark-worker-b:
    container_name: SparkWorkerB
    build:
      context: ./spark/build
      dockerfile: Dockerfile
    ports:
      - "9092:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - server

networks:
  server:

volumes:
  db_volume:
  minio_volume:
